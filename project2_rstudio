---
title: "Project 2 Report"
output: html_document
---

## Group Members

Specify the names and EIDs of all group members (if not working in a group just put your own name and EID): Joseph Choi (jc96676), Dana (ddl2249), Jonathan Choi (jhc3264)



## Question

State question being addressed and describe the outcome variable to be predicted. Explain why it is useful to answer this question.

**Could the nutrient ID of a food item determine whether it is plant-based or animal-based?.**


## Data and Setup

Load the dataset into R below. In addition, load any packages that you will need using the `library()` function. For example, you will need to load the `tidyverse` and `tidymodels` packages.


```{r}
## Add your code for reading in the data here
library(tidyverse)
library(tidymodels)
food_df <- read_csv("food_data.csv")

```


## Base Prediction Model

Use either linear regression (for continuous outcome) or logistic regression (binary outcome) to build a base prediction model. Use the tidymodels framework to 

1. Split the dataset into a training and testing dataset using 75% of observations for training and 25% for testing; leave the test dataset aside for now.

2. Select variables to include in your model recipe. You may need to experiment with different variable combinations to find the best predicting model.

Use 10-fold cross-validation on the training data to estimate either 

* the root mean squared error (RMSE) if your outcome is continuous; or 
* the accuracy if your outcome is binary.


```{r}
## Add your code here
long_food <- read_csv("food_data.csv", show_col_types = FALSE) |>
  filter(!is.na(nutrient_id))            

plant_groups <- c(
  "Legumes and Legume Products", "Vegetables and Vegetable Products",
  "Fruits and Fruit Juices", "Cereal Grains and Pasta",
  "Nut and Seed Products", "Spices and Herbs"
)

long_food <- long_food |>
  mutate(
    is_plant_based = if_else(
      description_category %in% plant_groups, "plant", "animal"
    ),
    is_plant_based = factor(is_plant_based, levels = c("animal", "plant"))
  ) |> 
  drop_na(is_plant_based)
ids_top <- long_food |> 
  distinct(fdc_id, nutrient_id) |>
  count(nutrient_id, sort = TRUE) |> 
  slice_head(n = 50) |> 
  pull(nutrient_id)
wide_food <- long_food |> 
  filter(nutrient_id %in% ids_top) |> 
  pivot_wider(
    id_cols      = c(fdc_id, is_plant_based),
    names_from   = nutrient_id,
    names_prefix = "n",
    values_from  = amount,
    values_fill  = 0                     
  )

set.seed(123)
split_data <- initial_split(food_wide, prop = 0.75, strata = is_plant_based)
train_data <- training(data_split)
test_data  <- testing(data_split)

recnet <- recipe(is_plant_based ~ ., data = data_train) |> 
  update_role(fdc_id, new_role = "ID") |> 
  step_zv(all_numeric_predictors()) |>      
  step_normalize(all_numeric_predictors())

specenet <- logistic_reg(
    penalty = tune(),
    mixture = tune()
  ) |> 
  set_engine("glmnet") |> 
  set_mode("classification")

wf_enet <- workflow() |> 
  add_recipe(recnet) |>
  add_model(specenet)
set.seed(123)
folded_cv <- vfold_cv(train_data, v = 10, strata = is_plant_based)

grid_lambda <- grid_regular(
  penalty(range = c(-4, 0)),  
  mixture(range = c(0, 1)),   
  levels = 5
)

set.seed(123)
recnet <- tune_grid(
  wf_enet,
  resamples = folded_cv,
  grid      = grid_lambda,
  metrics   = metric_set(accuracy, roc_auc),
  control   = control_grid(save_pred = TRUE)
)

collect_metrics(recnet) %>% arrange(desc(mean))

best_recnet <- select_best(recnet, metric = "roc_auc")
wf_final <- finalize_workflow(wf_enet, best_recnet)

set.seed(123)
res_test <- last_fit(wf_final, split_data)
collect_metrics(res_test)

library(vip)
fit_final <- extract_fit_parsnip(res_test$.workflow[[1]])
vip(fit_final, num_features = 20)
```

Which configuration of variables in your model seems to produce the best performing model on the training data?

**In this model, variables that produce the best performing model on the training data include n1253, n1263, n1262, n1102, and n1264.**



## Alternate Prediction Model

For this part of the analysis, choose one machine learning approach to build an alternate prediction model. The ones we have seen in class are K-Nearest Neighbors (`nearest_neighbor()`), decision trees (`decision_tree()`), and random forests (`rand_forest()`). 

1. Identify the tuning parameters for your chosen model.

2. Explore which combination of variables and tuning parameters produces good model performance.

3. Create a workflow that assembles your recipe and your model specification.

4. Use 10-fold cross-validation to estimate either the RMSE (continous outcome) or the accuracy (binary outcome) for your model in the training data.

5. Tune your model using a range of possible tuning parameters to identify the best-predicting model

NOTE: Doing the cross-validation and model tuning here may take a *significant* amount of time depending on your choice of tuning parameters and the speed of your computer. You may need to be patient with the process here.


```{r}
## Add your code here
library(tidymodels)
library(ranger)
# Random Forest model specification
model <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("ranger") %>%
  set_mode("classification")

# Create a workflow 
wf <- workflow() %>%
  add_recipe(recnet) %>% 
  add_model(model)

grid_rf <- grid_regular(
  mtry(range = c(2, 10)),
  trees(range = c(100, 300)),
  min_n(range = c(2, 5)),
  levels = 2 
)

set.seed(123)
tune_rf <- tune_grid(
  wf,
  resamples = folded_cv,
  grid = grid_rf,
  metrics = metric_set(accuracy, roc_auc),
  control = control_grid(save_pred = TRUE)
)

collect_metrics(tune_rf) %>% arrange(desc(mean))


# Shows the best performing Random Forest model based on ROC AUC
rf_best <- select_best(tune_rf, metric = "roc_auc")
rf_best

# Final workflow with the best Random Forest parameters
final_wf_rf <- finalize_workflow(wf, rf_best)

# Using training folds evalulate the performance
results_train_rf <- fit_resamples(
  final_wf_rf,
  resamples = folded_cv,
  metrics = metric_set(accuracy, roc_auc)
)

collect_metrics(results_train_rf)

# Choose the best performing workflow based on the cross-validation results

best_workflow <- final_wf_rf

# Evaluate the best workflow 
set.seed(123)
bestworkflow_final <- last_fit(best_workflow, split_data)

# Collect the performance metrics 
collect_metrics(bestworkflow_final)
```

Which one of your models (i.e. which combination of tuning parameters and variables) produces the best prediction performance on the training dataset?

**The Random forest model with mtry = 10, trees = 100, and min_n = 2  produced the best prediction performance on the training datset with the mean ROC being 0.868 compared to the best tuned Logistic Regression Model ROC being 0.729..**


## Final Model Fit

Choose the model that performs the best on your training dataset (either the linear/logistic regression or the machine learning model). Make a final assessment of your model using the testing dataset.


```{r}
## Final Model Fit: Evaluate best model on test set
set.seed(123)
bestworkflow_final <- last_fit(best_workflow, split_data)

## Collect performance metrics on test data
final_metrics <- collect_metrics(bestworkflow_final)
final_metrics

```


What are the final performance metrics for your best model?

**The best-performing model is the random forest classifier with tuned parameters mtry= 10, trees= 100, and min_n= 2 with accuracy of 0.823, ROC AUC of 0.859, and brier score of 0.128.**



## Model Improvement


Suppose you had the ability to improve the fit of the model by collecting data on a new variable. What data would you most like to collect to improve the prediction performance of the model? Explain why you think collecting this new variable would improve the prediction performance. You may optionally include some code/analysis to support your explanation.

```{r}
## Optional: Add any supporting code here

```

**One valuable variable that could improve model performance is the processing method or form of the food itemâ€”such as "raw," "cooked," "dried," "powdered," or "processed." This information would help differentiate food items that might share similar nutrient compositions but originate from different sources. For instance, both plant-based protein powders and animal-based protein powders may have similar levels of protein, but their processing methods and origin types differ. Including the processing method could improve prediction performance by providing additional context to nutrient values, or helping the model resolve ambiguous cases where nutrient ID patterns overlap between plant and animal categories.**




## Discussion

Reflect on the process of conducting this project. What was challenging, what have you learned from the process itself? Was there anything that was unexpected that you found during this analysis? If so, what was your expectation and how did the experience deviate from your expectation?

**The challenging aspects of this project included tuning parameters such as mtry(), trees(), min_n(). In order to provide the best tuning parameter combinations, there were many trials we had to run. Adjusting the parameters slightly required precision. I expected the Rain Forest model to work better compared to the Logistic Regression model since there were nonlinear relationships between nutrient ID and whether the product is plant based on animal based. The Rain Forest model was able to analyze the relationship between the two, and shows the the nutrient ID may depend on other factors such as the way the food was produced.  **





## Submission to Gradescope

**Make sure to add your group members to the Gradescope submission!**
