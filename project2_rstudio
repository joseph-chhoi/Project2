---
title: "Project 2 Report"
output: html_document
---

## Group Members

Specify the names and EIDs of all group members (if not working in a group just put your own name and EID): Joseph Choi (jc96676), Dana (ddl2249), Jonathan Choi (jhc3264)



## Question

State question being addressed and describe the outcome variable to be predicted. Explain why it is useful to answer this question.

**Could the nutrient ID of a food item determine whether it is plant-based or animal-based?.**


## Data and Setup

Load the dataset into R below. In addition, load any packages that you will need using the `library()` function. For example, you will need to load the `tidyverse` and `tidymodels` packages.


```{r}
## Add your code for reading in the data here
library(tidyverse)
library(tidymodels)
food_df <- read_csv("food_data.csv")

```


## Base Prediction Model

Use either linear regression (for continuous outcome) or logistic regression (binary outcome) to build a base prediction model. Use the tidymodels framework to 

1. Split the dataset into a training and testing dataset using 75% of observations for training and 25% for testing; leave the test dataset aside for now.

2. Select variables to include in your model recipe. You may need to experiment with different variable combinations to find the best predicting model.

Use 10-fold cross-validation on the training data to estimate either 

* the root mean squared error (RMSE) if your outcome is continuous; or 
* the accuracy if your outcome is binary.


```{r}
## Add your code here
long_food <- read_csv("food_data.csv", show_col_types = FALSE) |>
  filter(!is.na(nutrient_id))            

plant_groups <- c(
  "Legumes and Legume Products", "Vegetables and Vegetable Products",
  "Fruits and Fruit Juices", "Cereal Grains and Pasta",
  "Nut and Seed Products", "Spices and Herbs"
)

long_food <- long_food |>
  mutate(
    is_plant_based = if_else(
      description_category %in% plant_groups, "plant", "animal"
    ),
    is_plant_based = factor(is_plant_based, levels = c("animal", "plant"))
  ) |> 
  drop_na(is_plant_based)
ids_top <- long_food |> 
  distinct(fdc_id, nutrient_id) |>
  count(nutrient_id, sort = TRUE) |> 
  slice_head(n = 50) |> 
  pull(nutrient_id)
wide_food <- long_food |> 
  filter(nutrient_id %in% ids_top) |> 
  pivot_wider(
    id_cols      = c(fdc_id, is_plant_based),
    names_from   = nutrient_id,
    names_prefix = "n",
    values_from  = amount,
    values_fill  = 0                     
  )

set.seed(123)
split_data <- initial_split(food_wide, prop = 0.75, strata = is_plant_based)
train_data <- training(data_split)
test_data  <- testing(data_split)

recnet <- recipe(is_plant_based ~ ., data = data_train) |> 
  update_role(fdc_id, new_role = "ID") |> 
  step_zv(all_numeric_predictors()) |>      
  step_normalize(all_numeric_predictors())

specenet <- logistic_reg(
    penalty = tune(),
    mixture = tune()
  ) |> 
  set_engine("glmnet") |> 
  set_mode("classification")

wf_enet <- workflow() |> 
  add_recipe(recnet) |>
  add_model(specenet)
set.seed(123)
folded_cv <- vfold_cv(train_data, v = 10, strata = is_plant_based)

grid_lambda <- grid_regular(
  penalty(range = c(-4, 0)),  
  mixture(range = c(0, 1)),   
  levels = 5
)

set.seed(123)
recnet <- tune_grid(
  wf_enet,
  resamples = folded_cv,
  grid      = grid_lambda,
  metrics   = metric_set(accuracy, roc_auc),
  control   = control_grid(save_pred = TRUE)
)

collect_metrics(recnet) %>% arrange(desc(mean))

best_recnet <- select_best(recnet, metric = "roc_auc")
wf_final <- finalize_workflow(wf_enet, best_recnet)

set.seed(123)
res_test <- last_fit(wf_final, split_data)
collect_metrics(res_test)

library(vip)
fit_final <- extract_fit_parsnip(res_test$.workflow[[1]])
vip(fit_final, num_features = 20)
```

Which configuration of variables in your model seems to produce the best performing model on the training data?

**In this model, variables that produce the best performing model on the training data include n1253, n1263, n1262, n1102, and n1264.**



## Alternate Prediction Model

For this part of the analysis, choose one machine learning approach to build an alternate prediction model. The ones we have seen in class are K-Nearest Neighbors (`nearest_neighbor()`), decision trees (`decision_tree()`), and random forests (`rand_forest()`). 

1. Identify the tuning parameters for your chosen model.

2. Explore which combination of variables and tuning parameters produces good model performance.

3. Create a workflow that assembles your recipe and your model specification.

4. Use 10-fold cross-validation to estimate either the RMSE (continous outcome) or the accuracy (binary outcome) for your model in the training data.

5. Tune your model using a range of possible tuning parameters to identify the best-predicting model

NOTE: Doing the cross-validation and model tuning here may take a *significant* amount of time depending on your choice of tuning parameters and the speed of your computer. You may need to be patient with the process here.


```{r}
## Add your code here

```

Which one of your models (i.e. which combination of tuning parameters and variables) produces the best prediction performance on the training dataset?

**Write your answer here.**


## Final Model Fit

Choose the model that performs the best on your training dataset (either the linear/logistic regression or the machine learning model). Make a final assessment of your model using the testing dataset.


```{r}
## Add your code here

```


What are the final performance metrics for your best model?

**Write your answer here.**



## Model Improvement


Suppose you had the ability to improve the fit of the model by collecting data on a new variable. What data would you most like to collect to improve the prediction performance of the model? Explain why you think collecting this new variable would improve the prediction performance. You may optionally include some code/analysis to support your explanation.

```{r}
## Optional: Add any supporting code here

```

**Write your answer here.**




## Discussion

Reflect on the process of conducting this project. What was challenging, what have you learned from the process itself? Was there anything that was unexpected that you found during this analysis? If so, what was your expectation and how did the experience deviate from your expectation?

**Write your answer here.**





## Submission to Gradescope

**Make sure to add your group members to the Gradescope submission!**
